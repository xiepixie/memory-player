# 状态管理设计（Memory Player）

本文记录当前项目在 **全局状态管理** 与 **数据同步层（Supabase/Mock）** 上的技术设计，主要依据：

- [src/store/appStore.ts](cci:7://file:///d:/memory-player/src/store/appStore.ts:0:0-0:0)
- [src/lib/storage/SupabaseAdapter.ts](cci:7://file:///d:/memory-player/src/lib/storage/SupabaseAdapter.ts:0:0-0:0)
- [src/lib/storage/types.ts](cci:7://file:///d:/memory-player/src/lib/storage/types.ts:0:0-0:0)
- 以及辅助文档 [docs/DEVELOPER_GUIDE_FSRS_FLOW.md](cci:7://file:///d:/memory-player/docs/DEVELOPER_GUIDE_FSRS_FLOW.md:0:0-0:0)

---

## 1. 技术栈与总体思路

- **状态管理库**：Zustand
  - 单一全局 store：`useAppStore`。
  - 通过 `devtools` + `persist` 中间件增强：
    - `devtools`：配合浏览器 DevTools 调试。
    - `persist`：持久化少量 UI/路径配置（如 `rootPath`、`recentVaults`、`files`、`theme`），以及部分轻量数据层状态（如 `fileMetadatas`、`lastServerSyncAt`），用于离线/乐观展示。
- **数据服务抽象**：[DataService](cci:2://file:///d:/memory-player/src/lib/storage/types.ts:36:0-127:1) 接口
  - 由 `MockAdapter`（本地开发/离线）与 [SupabaseAdapter](cci:2://file:///d:/memory-player/src/lib/storage/SupabaseAdapter.ts:7:0-892:1)（云端）实现。
  - 上层 `appStore` 不关心具体实现，只通过统一方法读写元数据与 FSRS 状态。
- **FSRS 调度粒度**：
  - 一条 **Card = 一个 Cloze**。

**核心原则**：
- `useAppStore` 负责 App 级别的 UI/Session 状态与 DataService 调用。
- [DataService](cci:2://file:///d:/memory-player/src/lib/storage/types.ts:36:0-127:1) 负责「笔记 → 卡片 → FSRS 状态 → 历史」的持久化与同步。

---

## 2. AppState 结构总览

[AppState](cci:2://file:///d:/memory-player/src/store/appStore.ts:21:0-93:1) 将状态按领域拆分为多个 slice：

- **ServiceSlice**：当前数据服务、登录/同步信息。
- **VaultSlice**：本地库（Vault）、文件列表、元数据、内容缓存、Note ID 映射.
- **HistorySlice**：复习历史与待同步计数.
- **SessionSlice**：当前复习 Session 队列与统计、打分逻辑.
- **NoteSlice**：当前打开笔记的内容与 Metadata，以及当前聚焦的 `clozeIndex`.
- **UISlice**：视图模式（library/review/test/master/edit/summary）、主题等 UI 偏好.
- **SmartQueueSlice**：基于 FSRS 的到期队列、搜索、挂起/重置等高级操作.

下文按 slice 逐一说明关键字段与方法。

---

## 3. ServiceSlice：数据服务与认证

来源：[createServiceSlice](cci:1://file:///d:/memory-player/src/store/appStore.ts:181:0-256:3)。

### 3.1 状态字段

- `dataService: DataService`
  - 当前使用的后端实现（`MockAdapter` 或 [SupabaseAdapter](cci:2://file:///d:/memory-player/src/lib/storage/SupabaseAdapter.ts:7:0-892:1)）。
- `syncMode: 'mock' | 'supabase'`
  - 标记当前运行模式，影响一些统计字段（如 `pendingSyncCount`）。
- `currentUser: { id: string; email?: string | null } | null`
  - Supabase 模式下的当前用户信息.
- `lastSyncAt: Date | null`
  - 最近一次成功同步（或成功保存复习）的时间戳.
- `lastServerSyncAt: string | null`
  - 最近一次从后端拉取元数据时，服务端返回的时间游标（由 `DataService.getAllMetadata` 的 `serverNow` 提供）。
  - 作为增量同步的 `after` 参数使用，尽量避免仅依赖本地时间导致的时钟漂移漏数.
- `authCheckCounter: number`
  - 一个简单的计数器，用于触发依赖 `useEffect` 的重新认证检查.

### 3.2 核心方法

- [initDataService(type: 'mock' | 'supabase')](cci:1://file:///d:/memory-player/src/store/appStore.ts:189:2-221:3)
  - 动态 `import` 对应的 Adapter 并调用 [init()](cci:1://file:///d:/memory-player/src/lib/storage/types.ts:37:2-40:23)。
  - 若为 Supabase 模式，尝试通过 Supabase 客户端获取当前用户并写入 `currentUser`.
  - 完成后仅更新 `dataService` / `syncMode` / `currentUser` / `lastSyncAt`，**不再在函数内部主动调用** `loadVaults` / `loadAllMetadata` / `loadReviewHistory`，以保证启动关键路径尽量轻量；后续数据加载交由 LibraryView / Dashboard 等视图按需触发.
  - 若存在 Supabase 会话，还会在后台做一次 `auth.getUser()` 复检，失败时触发自动登出与 Toast 提示.
- [signOut()](cci:1://file:///d:/memory-player/src/store/appStore.ts:227:2-251:3)
  - 登出 Supabase 并强制退回本地 Mock 模式，重置所有库相关状态.

---

## 4. VaultSlice：Vault、文件与元数据

来源：[createVaultSlice](cci:1://file:///d:/memory-player/src/store/appStore.ts:258:0-458:3)。

### 4.1 状态字段

- `rootPath: string | null`：当前 Vault 根路径.
- `files: string[]`：当前 Vault 下的文件列表.
- `fileMetadatas: Record<string, NoteMetadata>`：以 `filepath` 为 key 的笔记元数据（含 FSRS 卡片状态）。
- `idMap` / `pathMap`：`noteId ↔ filepath` 的双向映射.
- `contentCache`：LRU 策略的文件内容缓存（上限 200 条）。
- `vaults` / `currentVault`：Vault 列表与当前选中项.

### 4.2 核心方法

- [setRootPath(path)](cci:1://file:///d:/memory-player/src/store/appStore.ts:290:2-300:3)
  - 切换 Vault，维护 `recentVaults` 历史，重置 `contentCache`.
- [loadAllMetadata()](cci:1://file:///d:/memory-player/src/store/appStore.ts:455:2-523:3)
  - 依赖 `currentVault`：若未选中 Vault 则跳过加载，避免对所有用户数据做全表扫描.
  - 使用 `lastServerSyncAt` 作为「增量游标」：
    - 若 `lastServerSyncAt === null`：视为**首次全量同步**（Full Sync）。
    - 否则：视为**增量同步**（Incremental Sync），仅请求自上次游标之后有变动的卡片/笔记.
  - 调用 `dataService.getAllMetadata(currentVault.id, after)`：
    - 返回 `{ items: NoteMetadata[], serverNow: string }`：
      - `items`：一批受影响的笔记元数据；
      - `serverNow`：由适配器计算出的最新更新时间戳，用于更新 `lastServerSyncAt`.
  - 合并策略（伪代码）：
    - 初始化 `next = { ...state.fileMetadatas }`；
    - 遍历远端 `items`：
      - 若 `m.isDeleted === true`：
        - 视为「整篇笔记被软删」，删除 `next[m.filepath]`（若存在），然后跳过该条；
      - 否则：
        - 若本地不存在该文件：`next[m.filepath] = m`；
        - 若本地已存在：
          - 创建 `mergedCards = { ...existing.cards }`；
          - 遍历 `m.cards`：
            - 若某条 `card` 带有 `(card as any).isDeleted === true`：从 `mergedCards` 中删除对应 `clozeIndex`（单卡软删）；
            - 否则：覆盖/插入该 `clozeIndex` 的 Card；
          - 生成新的 `next[m.filepath] = { ...existing, ...m, cards: mergedCards }`。
    - 最终 `set({ fileMetadatas: next, lastServerSyncAt: serverNow })`，将服务端游标前移.
  - 对于全量同步场景，目前实现倾向于**保守合并**：不主动删除「本地存在但远端未返回」的条目，以避免误删本地仅存在的草稿或 Demo 数据；真正的删除靠远端显式返回 `isDeleted` 的变更来驱动.
- [handleExternalCardUpdate(row)](cci:1://file:///d:/memory-player/src/store/appStore.ts:635:2-745:3) **[关键]**
  - 处理 Supabase Realtime 推送的 `cards` 表更新.
  - 逻辑：
    1. 将 `note_id` 映射回本地 `filepath`.
    2. 比较新旧 Card 的 FSRS 关键字段（state/stability/due）。
- `loadVaults()`
  - 从 DataService 拉取当前用户的所有 Vault 列表，并根据 `rootPath` / 既有 `currentVault` 推断默认选中项；
  - 写入 `vaults` / `currentVault` 后，若选中了 Vault，则立即调用 `loadAllMetadata()` 做一次懒加载或增量拉取；
  - 在 Supabase 模式下遇到 401 / PGRST301 时触发 `signOut()` 和统一的 “Session expired|Please log in again to continue syncing.” Toast。
- `setCurrentVault(vault)`
  - 切换当前 Vault 选择；当 `vault.config?.rootPath` 与当前 `rootPath` 不一致时：
    - 同步更新 `rootPath`、清空 `files` 与 `contentCache`，并将该路径加入 `recentVaults`；
  - 最后调用 `loadAllMetadata()`，确保 Library/Dashboard 立刻基于新 Vault 的元数据渲染.
- `createVault(name, config?)`
  - 通过 `dataService.createVault(name, config)` 在后端创建 Vault（Supabase 或 Mock）；
  - 完成后调用 `loadVaults()` 刷新本地 Vault 列表，并在成功创建时自动 `setCurrentVault(created)`，统一新 Vault 的选中与数据加载流程；
  - 由 `VaultSelector` / `LibraryView` 等组件调用，用于创建并绑定首个 Vault.
- `updateVault(id, updates)`
  - 通过 `dataService.updateVault(id, updates)` 更新现有 Vault（例如为已有 Vault 写入/更新 `config.rootPath`）；
  - 成功后调用 `loadVaults()` 重新拉取最新 Vault 配置，保持本地列表与服务端一致.
- `restoreNote(noteId)`
  - 作为「回收站恢复」的统一入口，由 `RecycleBin` 组件调用；
  - 内部通过 `dataService.restoreNote(noteId)` 取消云端软删标记，并调用 `updateLastSync()` 更新最近同步时间；
  - 若当前 `idMap` 中已知该 `noteId` 对应的 `filepath`，则只对该文件调用 `refreshMetadata(filepath, noteId)` 做精确刷新；
  - 否则在已选中 Vault 的前提下回退到一次 `loadAllMetadata()`，保证本地 `fileMetadatas` 与云端恢复状态对齐.

---

## 5. SessionSlice：复习队列与打分

来源：[createSessionSlice](cci:1://file:///d:/memory-player/src/store/appStore.ts:483:0-637:3)。

### 5.1 状态字段

- `queue: QueueItem[]`：当前复习队列（每项即一个 Cloze）。
- `sessionIndex`：当前进度索引.
- `sessionStats`：本轮复习的统计（计数、耗时、各档评分分布）。
- `isGrading`：防抖标记位.

### 5.2 核心方法

- [startSession()](cci:1://file:///d:/memory-player/src/store/appStore.ts:492:2-510:3)
  - 初始化统计，加载队列第一张卡片，进入 `test` 视图模式.
- [saveReview(rating: 1|2|3|4)](cci:1://file:///d:/memory-player/src/lib/storage/types.ts:69:2-77:91) **[FSRS 调度核心]**
  - 流程：
    1. 调用 `ts-fsrs` 的 `f.repeat(card, now)` 计算新状态.
    2. 乐观更新本地状态（`currentMetadata` + `fileMetadatas`）。
    3. 调用 [dataService.saveReview](cci:1://file:///d:/memory-player/src/lib/storage/types.ts:69:2-77:91) 持久化到后端.
    4. 记录 `lastLocalReview` 以过滤随后的 Realtime 回流.
    5. 自动加载下一张卡片或结束 Session.

---

## 6. NoteSlice：当前笔记

来源：[createNoteSlice](cci:1://file:///d:/memory-player/src/store/appStore.ts:639:0-789:3).

### 6.1 状态字段

- `currentFilepath`
- `currentNote`：解析后的结构（Frontmatter + Content + Clozes）。
- `currentMetadata`：当前笔记的 FSRS 数据.
- `currentClozeIndex`：**当前聚焦的题目**（为 null 时表示浏览整篇）。

### 6.2 核心方法

- [loadNote(filepath, targetClozeIndex)](cci:1://file:///d:/memory-player/src/store/appStore.ts:1015:2-1061:3)
  - 通过若干内部 helper 组合完成，逻辑上可拆为 4 步：
    1. **内容加载（loadContentFromSource）**：
       - 先查 `contentCache`，命中则直接返回；
       - 若当前 Vault 为 `DEMO_VAULT`，使用内置 Demo 模板生成 Markdown 内容，同时复用已有的 `pathMap[filepath]` 作为 `noteId`（若存在）；
       - 否则在桌面环境下调用 `fileSystem.ensureNoteId(filepath)`：
         - 返回最新文件内容；
         - 确保 Frontmatter 中存在稳定的 Note ID；
         - 若发现新的 Note ID，则后续会写入 `idMap` / `pathMap`.
    2. **缓存与 ID 映射更新（updateCacheAndIds）**：
       - 维护 `contentCache` 的 LRU 行为（上限 `MAX_CONTENT_CACHE_ENTRIES` 条）；
       - 只要拿到了合法的 `noteId`，就确保：
         - `idMap[noteId] = filepath`；
         - `pathMap[filepath] = noteId`；
       - 这一点保证了 Demo / 本地文件 / Supabase 三种场景下，Note ID 映射的行为保持一致.
    3. **解析与视图状态更新**：
       - 使用 `parseNote(content)` 解析 Markdown，得到 `currentNote`；
       - 根据当前全局 `viewMode` 选择目标模式：
         - 若当前模式属于 `['edit', 'test', 'master']`，则保持不变；
         - 否则进入 `review` 模式；
       - 将 `currentFilepath` / `currentNote` / `currentMetadata`（优先使用现有 `fileMetadatas[filepath]` 缓存） / `currentClozeIndex` 一次性写入 store；
       - 若 `targetClozeIndex` 非空且处于 `test`/`review` 模式，启动当前卡片的复习计时器 `currentReviewStartTime`，否则清空该计时.
    4. **元数据同步（syncNoteMetadata）**：
       - 调用 `dataService.getMetadata(noteId || '', filepath)` 获取该笔记的最新 FSRS 元数据；
       - 若已经握有 `noteId`，则强制写回 `metadata.noteId = noteId`；
       - 合并到 `fileMetadatas[filepath]`，并在 `currentFilepath === filepath` 时同步刷新 `currentMetadata`.

  - **对外语义**：
    - 仍然遵循「优先本地缓存，其次文件系统 / Demo 内容，最终从数据服务拉取 FSRS 状态」的流程；
    - 通过 helper 函数拆分复杂度，使得 `loadNote` 本身更易于单元测试和维护，但不会改变调用方对其行为的预期.

- `saveCurrentNote(content: string)`
  - 封装「本地保存 + 内存缓存更新 + 后台同步 + `refreshMetadata`」的一站式操作：
    - 调用 `fileSystem.writeNote(currentFilepath, content)` 将当前笔记写入本地；
    - 按 `MAX_CONTENT_CACHE_ENTRIES` 规则维护 `contentCache` 的 LRU 行为，并立即用 `parseNote(content)` 刷新 `currentNote`；
    - 若存在 `pathMap[currentFilepath]` 与有效 `dataService`：
      - 调用 `dataService.syncNote(currentFilepath, content, noteId, currentVault?.id)` 将内容同步到后端；
      - 成功时更新 `lastSyncAt`、调用 `markNoteSynced` 并通过 `refreshMetadata(filepath, noteId)` 刷新该笔记的 FSRS 元数据；
      - 失败时调用 `markNoteSyncPending` 并通过 Toast 提示 "Cloud sync failed (saved locally)"，保证本地保存不丢失的同时给出清晰反馈.
  - 目前主要由 `EditMode.handleSave` 调用，替代了组件内手写的保存 + 同步管线逻辑，使保存行为在 store 层集中管理.

---

## 7. DataService 与 SupabaseAdapter 实现

### 7.1 核心职责

SupabaseAdapter 将 FSRS 逻辑映射到关系型数据库：

1.  **Note 同步 ([syncNote](cci:1://file:///d:/memory-player/src/lib/storage/types.ts:49:2-52:94))**
    - 计算 `content_hash` 跳过未变动文件.
    - 将 Markdown 内容拆解为多个 Card 行（`flattenToCards`）。
    - **Upsert 策略**：
      - `note_id` + `cloze_index` 是唯一键.
      - 若内容变动较大（相似度 < 0.6），自动对 `stability` 施加惩罚.
      - 始终保留已有的 FSRS 调度状态（不覆盖 due/state）。
    - 对「不再出现在笔记中的旧 cloze」不再物理删除，而是：
      - 将对应 `cards` 行更新为 `is_deleted = true, updated_at = now()`；
      - 若整篇笔记完全没有卡片，则对该 Note 下所有 Card 执行软删.
    - 配合 `cards.is_deleted` 字段与部分唯一索引 `uq_card_identity_active (note_id, cloze_index) WHERE is_deleted = false`，实现：
      - 允许在原有 cloze 被软删后「重生」同一 `cloze_index`；
      - 通过 `updated_at` + `is_deleted` 支持增量删除同步.

2.  **复习保存 ([saveReview](cci:1://file:///d:/memory-player/src/lib/storage/types.ts:69:2-77:91))**
    - 使用 RPC `submit_review` 进行事务操作：
      - 更新 `cards` 表状态.
      - 插入 `review_logs` 表历史.

3.  **智能队列 ([getDueCards](cci:1://file:///d:/memory-player/src/lib/storage/types.ts:105:2-110:51))**
    - 筛选条件：`due <= now` AND `is_suspended = false` AND `is_deleted = false` AND `notes.is_deleted = false`.
    - **Vault 隔离**：支持传入 `vaultId`，通过 `notes!inner` 连接强制过滤属于该 Vault 的卡片.
    - 联表查询 `notes` 获取文件路径.

4.  **增量元数据拉取 ([getAllMetadata](cci:1://file:///d:/memory-player/src/lib/storage/types.ts:92:2-98:103))**
    - 统一由 `DataService` 暴露：`getAllMetadata(vaultId?: string, after?: string | Date | null): Promise<{ items: NoteMetadata[], serverNow: string }>`.
    - Supabase 实现：
      - 以 `cards` 为主表，`notes` 为联表：`from('cards').select('*, notes(...)')`；
      - 当 `after` 为空：
        - 仅返回「活跃」数据：`cards.is_deleted = false AND notes.is_deleted = false`；
      - 当 `after` 不为空（增量模式）：
        - 通过 `cards.updated_at > after` 过滤出自上次游标之后有变动的卡片/笔记.
      - 调用结果按 `note_id` 折叠为若干 `NoteMetadata`：
        - `NoteMetadata.isDeleted` 反映 `notes.is_deleted`，用于 VaultSlice 决定整篇笔记是否从缓存移除；
        - 每个 `cards[clozeIndex]` 上可能带有 `(card as any).isDeleted` 标记，用于 VaultSlice 的单卡级删除合并逻辑.
      - `serverNow` 目前实现为本次结果集中最大的 `cards.updated_at`，若结果为空则退化为客户端当前时间（仍存在少量时钟漂移风险，但比单纯使用本地时间更稳健）。

    - MockAdapter 实现：
      - 忽略 `vaultId`/`after` 参数，始终返回当前内存中所有未软删的条目；
      - `serverNow` 简单取本地 `new Date().toISOString()`，仅用作游标推进占位.

### 7.2 数据库表结构映射

- `notes`: 存储文件元数据（path, title, tags, hash）。
- `review_logs`: 存储每次打分的详细记录.
- `vaults`: 存储用户库配置.

## 8. 典型数据流

1.  **启动**：`AuthGate` 调用 [initDataService](cci:1://file:///d:/memory-player/src/store/appStore.ts:189:2-221:3) 仅初始化 `dataService` 与解析当前用户；用户进入 `LibraryView` 后，由 `LibraryView` 通过 `loadSettings` / [loadVaults](cci:1://file:///d:/memory-player/src/store/appStore.ts:604:2-626:3) 选定 `currentVault`，再由 [loadAllMetadata](cci:1://file:///d:/memory-player/src/store/appStore.ts:455:2-523:3) 基于 `currentVault` + `lastServerSyncAt` 懒加载/增量拉取元数据；进入 Dashboard 时，由 `Dashboard` 通过 [loadReviewHistory](cci:1://file:///d:/memory-player/src/store/appStore.ts:752:2-769:3) 懒加载最近 365 天复习历史.
2.  **复习**：`Dashboard` 生成队列 -> [startSession](cci:1://file:///d:/memory-player/src/store/appStore.ts:826:2-843:3) -> [loadNote](cci:1://file:///d:/memory-player/src/store/appStore.ts:1170:2-1207:3)（内部通过 `loadContentFromSource` / `updateCacheAndIds` / `syncNoteMetadata` 完成加载与同步）-> 用户打分 -> [saveReview](cci:1://file:///d:/memory-player/src/lib/storage/types.ts:69:2-77:91) (FSRS calc) -> RPC 写库 -> 加载下一张.
3.  **同步**：本地保存文件（内置编辑器或外部编辑器） -> `useVaultWatcher` / `useFileWatcher` 监听到 `.md` 变更 -> 通过 `fileSystem.ensureNoteId(path)` 获取 `id` 和最新内容，并调用模块级 helper [syncNoteFromFilesystem](cci:1://file:///d:/memory-player/src/store/appStore.ts:385:0-398:3) -> 由 store 统一执行 `updateCacheAndIds` + `dataService.syncNote` + `updateLastSync` + `markNoteSynced` + `refreshMetadata`；在 Supabase 模式下，如因网络等原因产生了 `pendingNoteSyncs`，用户也可以通过 `manualSyncPendingNotes()` 一次性重试所有待同步文件，内部在重试完毕后会统一调用 `loadAllMetadata()` 与 `fetchDueCards(50)` 并更新 `lastSyncAt`，保证 Dashboard / Library 与云端状态快速对齐.
4.  **多端**：端 A 复习 -> Supabase 更新 -> 端 B 收到 Realtime -> [handleExternalCardUpdate](cci:1://file:///d:/memory-player/src/store/appStore.ts:635:2-745:3) -> 更新端 B 内存状态.

### 8.1 UI 组件与 Vault/Note actions 协作示例

- **RecycleBin（回收站恢复）**
  - UI 通过 `useAppStore((s) => s.restoreNote)` 订阅统一的 `restoreNote(noteId)` action；
  - 点击「Restore」按钮时，只负责：
    - 调用 `restoreNote(noteId)` 触发云端软删恢复 + 元数据刷新；
    - 在本地 `items` 数组中移除该条记录，并展示 Toast；
  - 是否需要重新拉取元数据、如何维护 `idMap` / `fileMetadatas` 由 VaultSlice 内部负责，UI 不直接关心具体同步细节.
- **VaultSelector（库选择与绑定）**
  - 通过 selector 读取 `vaults` / `currentVault` / `setCurrentVault` / `rootPath`，以及 `createVault` / `updateVault` 两个 action；
  - 创建新 Vault：
    - 调用 `createVault(newVaultName, { rootPath })`，由 store 统一完成「后端创建 -> 刷新列表 -> 选中新 Vault -> 懒加载元数据」；
    - 组件只负责管理输入框状态与创建成功/失败的 Toast；
  - 绑定当前文件夹到已有 Vault：
    - 调用 `updateVault(currentVault.id, { config: { ...currentVault.config, rootPath } })`；
    - 具体持久化及后续 `loadVaults()` 调用均在 VaultSlice 内部封装.
- **LibraryView（首次 Vault Onboarding）**
  - 在首次选择文件夹且尚无 Vault 时，调用 `createVault(defaultName, { rootPath })` 作为一键「创建并绑定」入口；
  - 成功后仅负责：
    - 显示 `"Vault \"name\" created & linked"` 的 Toast；
    - 标记本地 `vaultOnboarded`（用于控制引导 UI）；
  - Vault 的实际创建、当前 Vault 切换以及后续元数据加载同样由 VaultSlice 中的 action 统一处理，保持 UI 与状态管理层的解耦.

---

## 9. 单文件元数据刷新：refreshMetadata

### 9.1 设计动机

早期版本中，`VaultWatcher` 在监听到文件变更并调用 `syncNote` 后，**并不会立即刷新该文件对应的 `fileMetadatas`**，导致：

- Supabase 队列（`getDueCards`）已经包含新卡 / 更新后的 due;
- 但本地 `Dashboard` / `LibraryView` 仍使用旧的 `fileMetadatas`，`Due Today` 分组不会实时更新.

为解决这一不一致，引入全局 helper：

```ts
refreshMetadata(filepath: string, noteIdOverride?: string): Promise<void>
```

### 9.2 行为定义

`refreshMetadata` 位于 `VaultSlice` 中，职责：

- 使用 `dataService.getMetadata(noteId, filepath)` 拉取**单个笔记**的最新 FSRS 元数据;
- 维护以下不变式：
  - `files` 一定包含 `filepath`;
  - `idMap[noteId] = filepath`;
  - `pathMap[filepath] = noteId`;
  - `fileMetadatas[filepath]` 更新为最新的 `NoteMetadata`（并补上 `noteId`）。

Note ID 的推断顺序：

1. 调用方传入的 `noteIdOverride`;
2. 现有的 `pathMap[filepath]`;
3. 现有的 `fileMetadatas[filepath]?.noteId`;
4. 否则为空字符串（由后端自行解析 / 容错）。

### 9.3 VaultWatcher 的使用方式

`useVaultWatcher` 在监听到 `.md` 文件变更时：

1. 调用 `fileSystem.ensureNoteId(path)` 获取 `id` 和最新内容；
2. 通过模块级 helper [syncNoteFromFilesystem](cci:1://file:///d:/memory-player/src/store/appStore.ts:385:0-398:3) 触发统一的「更新缓存 + 同步到后端 + 更新 `lastSyncAt` + 标记已同步 + `refreshMetadata`」管线，避免在 hook 内重复实现.
3. 对删除事件（`kind === 'remove'`），则调用模块级 [softDeleteNoteForPath](cci:1://file:///d:/memory-player/src/store/appStore.ts:391:0-398:3)：基于 `pathMap` 解析 `noteId` 并调用 `dataService.softDeleteNote(noteId)`，再更新 `lastSyncAt`，实现外部删除 → 云端软删的闭环.

这样，**外部编辑器（如 Obsidian）保存后的新卡/更新卡，会立即出现在 Dashboard 的 `Due Today` 和 Library 的分组中**，无需再全量调用 `loadAllMetadata`.

---

## 10. 分组规则（Dashboard / Library）

### 10.1 Dashboard（`Dashboard.tsx`）

`dashboardData` 聚合时，对每张卡的处理规则：

- 统计 FSRS 状态：
  - `state === 0` 记入 `stats.new`;
  - `state === 1/2/3` 依次计入 `learning/review/relearning`;
- **新卡 (state === 0)**：
  - 永远计入 `newItems`;
  - 若 `due <= now`：
    - 同时计入 `dueItems`;
    - 若 `differenceInDays(now, due) >= 1` 再计入 `overdueItems`;
    - `futureCounts[today]++`;
  - 若 `due > now`：
    - 仅更新 `futureCounts[format(due, 'yyyy-MM-dd')]++`;
- **已学习/复习中的卡 (state > 0)**：
  - 若 `due <= now`：计入 `dueItems` / `overdueItems`;
  - 若 `due > now`：仅计入 `futureCounts[dateKey]`.

> 结果：**“今天到期的新卡” 同时出现在 `New` 和 `Due Today` 统计中**，更符合用户对「今天任务量」的直觉.

### 10.2 LibraryView（`LibraryView.tsx`）

Library 的文件分组基于每个文件下所有卡片：

- 预处理：
  - 若该文件没有 `meta.cards` 或卡片数组为空 → 视为 `New`;
- 对每张卡：
  - 若没有 `due` 或 `due` 非法：
    - 若 `reps === 0`，视为 `New`;
  - 否则解析 `due` 为日期：
    - 若 `reps === 0`：
    - 若 `reps > 0`：
      - `isPast(due) && !isToday(due)` → `hasOverdue = true`;
      - `isToday(due)` → `hasToday = true`.
 - 最终分组优先级:
   1. `hasOverdue` → `Overdue`;
   2. 否则 `hasToday` → `Due Today`;
   3. 否则 `hasNew` → `New Cards`;
   4. 否则 → `Library`（未来到期）。

 这样，“第一次同步的新卡，只要 due 在今天或过去”，会被归入 `Due Today` 而不是长期停留在 `New Cards`，与 Dashboard 显示保持一致.

 ## 11. 基于 Zustand 的状态管理优化计划

 ### 11.1 总体方向（对照官方最佳实践）

 - **统一使用 slice + actions 管理业务逻辑**
   - 继续保持当前基于 `createServiceSlice` / `createVaultSlice` / `createSessionSlice` / `createNoteSlice` 等的切片结构，使每个 slice 负责一个清晰的领域.
   - 避免在组件中直接拼装复杂业务流程（例如 FSRS 调度、保存 + 同步管线），而是通过 store actions 或模块级 helper 统一封装，方便测试与演进.

 - **用 selector 优化订阅范围**（官方推荐模式）
   - 尽量避免在组件中直接调用 `useAppStore()`（无 selector），这会订阅整个 AppState，任何字段变动都触发重渲染.
   - 推荐模式：`useAppStore((s) => s.currentNote)` 或 `useAppStore(selector, shallow)` 只订阅必要字段，尤其在 EditMode / Review / Dashboard 等高频渲染视图中收益明显.

 - **区分全局状态与局部 UI 状态**
   - 像 `EditMode` 中的 `activePreviewCloze`、`showShortcuts`、光标位置等纯 UI 状态，继续保留在组件内部（已符合最佳实践）。
   - 只把「跨视图共享、需要持久化或和数据服务强相关」的状态放进 `useAppStore`（如队列、FSRS 元数据、当前 note 元信息等）。

 ### 11.2 appStore.ts 优化方向

 - **[Selector + shallow 渲染优化]**
   - 目前：`EditMode` 已基本改为通过多个 selector（如 `currentNote` / `currentFilepath` 等）读取全局状态，重计算逻辑仍集中在组件本地 `useState` / `useMemo` / `useEffect` 中，整体行为与本节规划一致；其它高频组件（如 Dashboard / Review Player）可继续按此模式演进.
 - **[集中保存/同步流程到 Note/Vault slice]**
   - 目前：在 `createNoteSlice` 中已经实现 `saveCurrentNote(content: string)`，封装「本地写盘 + LRU 缓存更新 + 后台同步 + `refreshMetadata`」的完整管线，`EditMode` 已经迁移为直接调用该 action；外部编辑器场景则通过模块级 helper `syncNoteFromFilesystem` 触发相同的同步 + 刷新逻辑，避免在 hook 内重复实现.
   - 后续：如需支持更复杂的同步场景（批量重试、强制重拉元数据等），可以在 store 层继续扩展相应的 action / helper，而不再在 UI 层手工拼装流程.

 - **[模块级 actions 收敛 useAppStore.setState/getState]**
   - 目前：在部分组件中仍存在直接调用 `useAppStore.setState` / `useAppStore.getState()` 的情况，但对于与文件系统强相关的流程（如 VaultWatcher），已经通过 `syncNoteFromFilesystem` / `softDeleteNoteForPath` 等模块级 helper 收敛到 store 模块内部.
   - 计划：后续可以继续在 `appStore.ts` 中集中定义并导出更细粒度的模块级函数，例如：`export const markNoteSynced = (filepath: string) => useAppStore.getState().markNoteSynced(filepath);`、`export const refreshNoteMetadata = (filepath: string, noteId?: string) => useAppStore.getState().refreshMetadata(filepath, noteId);`。UI 层尽量不直接使用 `setState`，只通过 slice actions 或这些 helper 调用全局状态变更，减少状态更新逻辑的分散与重复.

 - **[FSRS / Session 领域边界进一步明确]**
   - 保持 FSRS 调度、session 队列推进、统计等逻辑完全收敛在 `SessionSlice` / `NoteSlice` 中：UI 只能通过 `startSession` / `saveReview` 等 action 驱动，这一点当前实现已经基本达成.后续若引入更复杂的操作（如「从队列头/尾插入卡片」「按标签过滤队列」），优先在 slice 内新增 action，而不是在组件层直接操作 queue 数组，以维持 store 的单一事实源.

 ### 11.3 EditMode.tsx 与全局 store 协作优化

- **[缩小 EditMode 的全局订阅范围]**
  - 目前：`EditMode` 已基本改为通过多个 selector（如 `currentNote` / `currentFilepath` 等）读取全局状态，重计算逻辑仍集中在组件本地 `useState` / `useMemo` / `useEffect` 中，整体行为与本节规划一致；其它高频组件（如 Dashboard / Review Player）可继续按此模式演进.

- **[统一保存入口，EditMode 只关心 UI 状态]**
  - 目前：`NoteSlice` 中已经提供 `saveCurrentNote(content: string)`：内部完成本地写盘、更新 `contentCache` / `currentNote`、触发 `syncNote` + `refreshMetadata`、更新 `pendingNoteSyncs` 与 `lastSyncAt`。`EditMode.handleSave` 已简化为：校验 `isDirty` 后调用 `saveCurrentNote(content)`，并根据结果管理 `isSaving` 状态和 toast 提示。好处：EditMode 不再依赖 `MAX_CONTENT_CACHE_ENTRIES` 等 store 内部实现细节；未来如需调整缓存策略或同步策略，只需修改 store 层.

- **[元数据刷新策略由 store 统一管理]**
  - 当前：`loadNote` 内通过 `syncNoteMetadata` 异步刷新 `currentMetadata` 与 `fileMetadatas`；`saveCurrentNote` 在成功同步后调用 `refreshMetadata`；`useVaultWatcher` 则通过 `syncNoteFromFilesystem` 统一走同一套刷新逻辑。整体上，FSRS 元数据的刷新时机已经基本由 store 统一管理，UI 只消费结果即可.

 ### 11.4 渐进式落地计划

- **短期（1–2 个迭代）**
  - 在 `EditMode`、Dashboard 等关键页面引入 selector 写法（目前已基本完成），继续替换历史上的 `useAppStore()` 无参调用.
  - 保持并演进统一的笔记保存 action（`saveCurrentNote`），将其它仍在组件内实现的保存/同步流程逐步迁移到 store.

 - **中期（3–5 个迭代）**
   - 系统性排查并收敛所有散落在组件中的 `useAppStore.setState/getState` 调用，替换为 slice actions 或 module-level helpers.
   - 在文档中补充各 slice 的领域边界约定，并标注「哪些状态可以进全局 store，哪些必须保持为局部 UI 状态」，形成团队统一的状态管理规范.
   - 评估是否需要按领域拆分为多个独立 store（例如 `useSessionStore`、`useLibraryStore` 等），或对复杂队列/过滤逻辑采用 redux-style reducer + `dispatch` 模式（zustand 官方支持）。
   - 在拆分或引入新模式时，保持对外 API 的稳定（通过门面函数/自定义 hooks 封装），避免对上层页面造成大规模改动.
 
 ## 12. 基于官方最佳实践的审阅结论
 
 本节基于 Zustand 官方文档（slices + devtools + persist + 自定义 storage）以及 supabase-js 官方指南，对当前 `appStore` / `SupabaseAdapter` / Supabase Schema 的实现做一次对照审阅，记录已经对齐的点与潜在优化方向.
 
 ### 12.1 Zustand：devtools + persist + slices 模式
 
 #### 12.1.1 已对齐的实践
 
 - **单一全局 store + slices 模式**
   - `useAppStore` 采用 `createServiceSlice` / `createVaultSlice` / `createSessionSlice` / `createNoteSlice` / `createUISlice` / `createSmartQueueSlice` 组合的 slices 结构，并通过 `AppStateCreator<T>` 统一类型参数，与官方 slices 模式示例基本一致.
   - 所有业务逻辑（FSRS 调度、同步管线、Vault 切换等）都通过 slice actions 暴露，UI 组件只调用 actions，不直接操作内部 state 结构.
 
 - **devtools + persist 组合位置正确**
   - 中间件在最外层统一包裹：`create<AppState>()(devtools(persist(...)))`，而不是在每个 slice 内单独使用，符合官方推荐的“在 bound store 外层一次性应用中间件”的模式.
   - `persist` 使用了 `partialize` 精准选择需要持久化的字段（如 `rootPath`、`recentVaults`、`fileMetadatas`、`viewMode`、当前 Note/Session 状态等），避免了整棵 AppState 的无差别持久化.
 
 - **迁移（migrate）版本化机制**
   - 通过 `version: 1 + migrate(persistedState, version)` 明确引入持久化版本，当 schema 变更（例如为增量同步引入 `lastServerSyncAt`）时，可以安全地清理过期缓存（如 `fileMetadatas`）。这一点与 Zustand 官方文档中关于 `version + migrate` 的最佳实践对齐.
 
 #### 12.1.2 潜在优化点（持久化与序列化）
 
 - **Date 等复杂类型的持久化问题**
   - 当前通过 `partialize` 将 `fileMetadatas` / `currentMetadata` / `currentNote` 等字段写入 `localStorage`。其中：
     - `fileMetadatas[filepath].cards[clozeIndex].due / last_review` 等字段是 `Date` 对象；
     - `currentMetadata` 本质上也是 `NoteMetadata`，同样包含 `Date`；
     - `currentNote` 体积可能较大（完整 Markdown 解析结构），但结构中目前主要是字符串/数组.
   - Zustand 默认的 `persist` 使用 `JSON.stringify` / `JSON.parse`，会把 `Date` 序列化为字符串。Hydrate 之后，这些字段类型退化为 `string`，而上层代码（Dashboard / LibraryView / `handleExternalCardUpdate` 等）普遍按 `Date` 使用（如 `card.due.getTime()`、`formatDistanceToNow(card.due)`）。
   - 目前在实际运行中，该问题部分被「每次启动后会重新从 Supabase 拉取最新元数据」所掩盖，但在 **离线模式 / 首屏渲染到首次云端刷新之间**，理论上存在类型不一致的风险.
 
 - **改进方向 A：采用自定义 storage 处理复杂类型**
   - 参考 Zustand 官方文档中集成 `Superjson` / `idb-keyval` 的示例，可以为 `persist` 提供自定义 `storage`：
     - 使用 `createJSONStorage(() => superjsonStorage)` 或类似实现，确保 `Date`、`Map`、`Set` 等在序列化/反序列化过程中保持类型不变；
     - 这样可以在不改变 `AppState` 结构的前提下，保证 `fileMetadatas` / `currentMetadata` 等字段在持久化往返后仍然是 `Card` / `ReviewLog` 期待的类型.
   - 结合现有规划，可以直接将 **存储后端切换到 IndexedDB**（例如基于 `idb-keyval` 实现的 `StateStorage`），同时用 `Superjson` 负责序列化，既解决容量问题，又解决类型问题.
 
 - **改进方向 B：收窄持久化的字段范围**
   - 若不希望在短期内引入额外依赖，也可以考虑收窄 `partialize` 的范围：
     - 不再持久化完整的 `fileMetadatas`，而是只持久化与 UI 布局/分组强相关、且类型简单的摘要字段（例如：每个文件的最后复习时间、卡片计数等）；
     - 对于 FSRS 细节（每张卡的 `due` / `stability` 等），完全依赖启动后的 `loadAllMetadata` / `refreshMetadata` 重新从 Supabase 拉取；
     - 这样可以显著减小 `localStorage` 占用，也避免了 `Date` 类型还原的问题，但会牺牲一部分「离线/秒开」体验，需要在产品层做权衡.
 
 - **当前结论**：
   - 就工程复杂度与长期维护成本而言，更推荐 **改进方向 A**：使用官方推荐的自定义 storage（IndexedDB + Superjson）统一解决「体积 + 类型」两个问题，再通过现有 `migrate` 机制平滑迁移已有存量数据.
 
 ### 12.2 SupabaseAdapter 与 Schema 的对齐情况
 
 #### 12.2.1 已对齐的实践
 
 - **软删除与增量同步模型**
   - Schema 中 `notes.is_deleted` / `cards.is_deleted` + 触发器维护的 `updated_at`，配合：
     - `SupabaseAdapter.softDeleteNote`：对 Note 与其所有 Card 统一标记软删；
     - `SupabaseAdapter.syncNote`：对不再出现的 cloze 执行软删，而不是物理删除；
     - `SupabaseAdapter.getAllMetadata(vaultId, after)`：
       - `after === null` 时只返回活跃（未软删）数据；
       - `after != null` 时基于 `cards.updated_at > after` 增量拉取，包含软删记录；
     - `VaultSlice.loadAllMetadata` 中的合并逻辑：
       - `NoteMetadata.isDeleted === true` 时整篇移除；
       - 单卡 `(card as any).isDeleted === true` 时从本地 `cards` 中删除该 cloze；
   - 这一整套「软删 + updated_at + 增量合并」的设计，与 Supabase 文档推荐的 **基于时间游标的增量同步模式** 高度一致.
 
 - **RLS 与多租户安全边界**
   - 所有核心表（`vaults` / `notes` / `cards` / `review_logs`）均开启 RLS，并按 `auth.uid()` 建立 Owner 级策略，Adapter 代码中也始终按「当前用户上下文」访问数据（`auth.getUser()`）。
   - 在查询层面，对于需要 vault 隔离的接口（`getVaults` / `getDueCards` / `searchCards` / `getAllMetadata` 等），都显式按 `vault_id` 过滤，进一步缩小数据可见范围.
 
 - **服务端时间与游标稳定性**
   - `getAllMetadata` 在结果非空时使用 `rows` 中的最大 `updated_at` 作为游标，在结果为空时回退到 `server_now()` RPC，避免依赖本地时钟；
   - 这一点与 Supabase 官方建议的「以服务端时间戳作为增量游标」做法一致.
 
 #### 12.2.2 潜在优化点
 
 - **增量游标的「相等时间戳」边界情况**
   - 当前条件为 `cards.updated_at > after`，并将本次批次中的最大 `updated_at` 作为新的 `after`：
     - 在极端情况下，如果有多条记录拥有相同的 `updated_at` 且恰好落在批次边界，则可能存在「最后一条记录在下次拉取中被跳过」的理论风险；
   - 可选增强方案：
     - 引入「(updated_at, note_id, cloze_index)」组成的复合游标，在客户端持久化这三个字段，在 SQL 中使用 `WHERE (updated_at, note_id, cloze_index) > (cursor_updated_at, cursor_note_id, cursor_cloze)`；
     - 或者简单地改为 `>=` 并在客户端做去重（按 `noteId + clozeIndex` 覆盖），以牺牲少量冗余带宽换取更强的健壮性.
   - 鉴于当前写入频率与触发器精度，该问题实际发生概率较低，因此可以作为 **中长期增强项** 记录在案.
 
 - **错误分类与复用**
   - 目前在多个 slice 中存在类似的 401 处理逻辑：`if (e.status === 401 || e.code === 'PGRST301') { signOut(); toast('Session expired') }`。
   - 可以考虑在 Adapter 层或一个 util 模块中集中实现 `isAuthError(e)` 辅助函数，统一处理 supabase-js 的错误对象结构，减少重复代码，并为后续接入错误翻译库（例如官方推荐的 error translator）预留扩展点.
 
 ### 12.3 总结
 
 - 当前 `appStore` + `SupabaseAdapter` + Schema 整体设计已经与 Zustand / Supabase 的官方最佳实践高度对齐，特别是在：
   - slices 模式、devtools/persist 的组合方式；
   - 软删除 + 时间游标驱动的增量同步；
   - RLS 与 vault 级数据隔离策略；
   - FSRS 状态只在后端持久化、在前端通过 actions 驱动的单向数据流.
 - 主要需要关注和后续迭代的点集中在两类：
   - **持久化与类型一致性**：通过自定义 storage（IndexedDB + Superjson）或收窄 `partialize` 范围，避免 `Date` 等复杂类型在 JSON 持久化过程中退化；
   - **增量游标边界与错误处理复用**：在现有设计基础上，进一步强化边界条件下的一致性与错误处理的集中化.
 
 这些结论将作为后续「状态管理与同步层」演进规划的补充依据，用于指导中长期的重构与优化决策.